{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome \u00b6 Welcome, browse sections at the top of the site. Roadmap \u00b6 Merge content from http://emergenceplatform.github.io/book/ Merge content from http://slate.is/docs Merge content from http://forum.laddr.us/ Merge content from http://forum.emr.ge/ Merge content from https://github.com/JarvusInnovations/emergence-docs (most dated)","title":"Welcome"},{"location":"#welcome","text":"Welcome, browse sections at the top of the site.","title":"Welcome"},{"location":"#roadmap","text":"Merge content from http://emergenceplatform.github.io/book/ Merge content from http://slate.is/docs Merge content from http://forum.laddr.us/ Merge content from http://forum.emr.ge/ Merge content from https://github.com/JarvusInnovations/emergence-docs (most dated)","title":"Roadmap"},{"location":"development/","text":"Development \u00b6 The Development section provides content covering: Overviews of the internal architecture and components Obtaining development environments Executing development workflows Feature implementation guides and examples","title":"Development"},{"location":"development/#development","text":"The Development section provides content covering: Overviews of the internal architecture and components Obtaining development environments Executing development workflows Feature implementation guides and examples","title":"Development"},{"location":"development/clone-from-git/","text":"Clone Laddr from git \u00b6 This guide is for developers who want to work on Laddr\u2019s core code. It will walk you through setting up a fresh site instance and cloning a version of Laddr into it from a remote git repository. Step 1: Obtain an emergence host \u00b6 You will need a host server dedicated to running emergence. If you don\u2019t have access to one already, the easiest way to get started is to spin up a small Ubuntu 16.04 LTS virtual machine with a cloud provider like Digital Ocean, Google Cloud Compute, AWS, or countless others. Once you are logged in to your fresh Ubuntu 16.04 machine, follow emergence\u2019s installation guide to prepare it for hosting emergence-powered sites like Laddr. Alternatively, if you\u2019re familiar with Docker, you can spin up an emergence container: docker run -d \\ -it \\ --name emergence \\ -v /emergence:/emergence \\ -p 127 .0.0.10:80:80 \\ -p 127 .0.0.10:3306:3306 \\ -p 127 .0.0.10:9083:9083 \\ jarvus/emergence \\ tmux new -s emergence emergence-kernel Step 2: Create a site for your laddr development instance \u00b6 Laddr is based on emergence\u2019s skeleton-v2 site template. Unlike when provising a deployment instance of Laddr, for development you want to create a site extending Laddr\u2019s parent site like Laddr does rather than Laddr itself. Laddr\u2019s code will be cloned from git and applied on top of the parent site. Use emergence\u2019s host control panel to create a new site with your desired hostname and initial user, just be sure to select skeleton-v2.emr.ge as the parent hostname. After the site is created login to /develop with your initial user developer account. Step 3: Configure mapping to the laddr git repository \u00b6 To configure a link between your emergence instance and a git repository, create a new file at php-config/Git.config.d/laddr.php and copy its initial contents from the the file at the same path in Laddr\u2019s develop branch on Github. Optionally, edit the remote option to point at your own fork, and switch it to the SSH protocol if you\u2019d like to be able to push changes from the web UI. Step 4: Initialize git repository \u00b6 Visit /site-admin/sources to initialize the configured git repository. If you switch the remote to an SSH git URL before initializing, a deploy key will be generated for you that you can install on GitHub before continueing to enable web-based read/write access. Otherwise, if you are cloning via HTTPS, you will need to SSH into the server and use the git CLI to push changes after initializing the repository. Step 5: Pull code from git \u00b6 Visit /site-admin/sources/laddr and click Pull if needed to pull the latest commits from github into your git working copy. Then click the Sync -> Update emergence VFS button to import the git working tree copy into your emergence instance. Next steps \u00b6 If you run into any trouble, need to reconfigure the repository, or execute any advanced maneuvers, use emergence-git-shell my-instance-name laddr on your host machine to drop into a properly-configured git shell where you can make full use of the git CLI client without any permissions issues.","title":"Clone Laddr from git"},{"location":"development/clone-from-git/#clone-laddr-from-git","text":"This guide is for developers who want to work on Laddr\u2019s core code. It will walk you through setting up a fresh site instance and cloning a version of Laddr into it from a remote git repository.","title":"Clone Laddr from git"},{"location":"development/clone-from-git/#step-1-obtain-an-emergence-host","text":"You will need a host server dedicated to running emergence. If you don\u2019t have access to one already, the easiest way to get started is to spin up a small Ubuntu 16.04 LTS virtual machine with a cloud provider like Digital Ocean, Google Cloud Compute, AWS, or countless others. Once you are logged in to your fresh Ubuntu 16.04 machine, follow emergence\u2019s installation guide to prepare it for hosting emergence-powered sites like Laddr. Alternatively, if you\u2019re familiar with Docker, you can spin up an emergence container: docker run -d \\ -it \\ --name emergence \\ -v /emergence:/emergence \\ -p 127 .0.0.10:80:80 \\ -p 127 .0.0.10:3306:3306 \\ -p 127 .0.0.10:9083:9083 \\ jarvus/emergence \\ tmux new -s emergence emergence-kernel","title":"Step 1: Obtain an emergence host"},{"location":"development/clone-from-git/#step-2-create-a-site-for-your-laddr-development-instance","text":"Laddr is based on emergence\u2019s skeleton-v2 site template. Unlike when provising a deployment instance of Laddr, for development you want to create a site extending Laddr\u2019s parent site like Laddr does rather than Laddr itself. Laddr\u2019s code will be cloned from git and applied on top of the parent site. Use emergence\u2019s host control panel to create a new site with your desired hostname and initial user, just be sure to select skeleton-v2.emr.ge as the parent hostname. After the site is created login to /develop with your initial user developer account.","title":"Step 2: Create a site for your laddr development instance"},{"location":"development/clone-from-git/#step-3-configure-mapping-to-the-laddr-git-repository","text":"To configure a link between your emergence instance and a git repository, create a new file at php-config/Git.config.d/laddr.php and copy its initial contents from the the file at the same path in Laddr\u2019s develop branch on Github. Optionally, edit the remote option to point at your own fork, and switch it to the SSH protocol if you\u2019d like to be able to push changes from the web UI.","title":"Step 3: Configure mapping to the laddr git repository"},{"location":"development/clone-from-git/#step-4-initialize-git-repository","text":"Visit /site-admin/sources to initialize the configured git repository. If you switch the remote to an SSH git URL before initializing, a deploy key will be generated for you that you can install on GitHub before continueing to enable web-based read/write access. Otherwise, if you are cloning via HTTPS, you will need to SSH into the server and use the git CLI to push changes after initializing the repository.","title":"Step 4: Initialize git repository"},{"location":"development/clone-from-git/#step-5-pull-code-from-git","text":"Visit /site-admin/sources/laddr and click Pull if needed to pull the latest commits from github into your git working copy. Then click the Sync -> Update emergence VFS button to import the git working tree copy into your emergence instance.","title":"Step 5: Pull code from git"},{"location":"development/clone-from-git/#next-steps","text":"If you run into any trouble, need to reconfigure the repository, or execute any advanced maneuvers, use emergence-git-shell my-instance-name laddr on your host machine to drop into a properly-configured git shell where you can make full use of the git CLI client without any permissions issues.","title":"Next steps"},{"location":"development/email/","text":"Working with Email \u00b6 Catching emails in development \u00b6 In the studio, you can enable relaying email to a specified SMTP endpoint. HELO and mailtrap are easy SMTP endpoints to set up for development that provide UIs for reviewing emails sent to all recipients. When emails are configured to be relayed to these services, they will be trapped for review and never actually delivered to anyone externally, no matter what recipients you use. As opposed to overriding recipient emails to test email features, this approach enables you to verify that personalized bulk emails send the right content to the right recipients. Using HELO \u00b6 Download and open the HELO app Launch studio, run start-all Install and activate postfix email backend: enable-email Configure postfix email backend to relay to EHLO app on Docker host machine: enable-email-relay host.docker.internal 2525 studio Or, using mailtrap: enable-email-relay smtp.mailtrap.io 2525 mailtrapusername mailtrappassword Sending a test email \u00b6 From the studio: Install netcat: hab pkg install --binlink core/netcat Open SMTP connection: nc localhost 25 Start SMTP session: EHLO localhost.localdomain Set sender: MAIL FROM: <sender@example.com> Set recipient: RCPT TO: <recipient@example.com> Set message: DATA Subject: Hello world! This is the body of my email. Have a good day. . Close SMTP session: QUIT Review postfix backend log: less -S /hab/cache/sys.log","title":"Working with Email"},{"location":"development/email/#working-with-email","text":"","title":"Working with Email"},{"location":"development/email/#catching-emails-in-development","text":"In the studio, you can enable relaying email to a specified SMTP endpoint. HELO and mailtrap are easy SMTP endpoints to set up for development that provide UIs for reviewing emails sent to all recipients. When emails are configured to be relayed to these services, they will be trapped for review and never actually delivered to anyone externally, no matter what recipients you use. As opposed to overriding recipient emails to test email features, this approach enables you to verify that personalized bulk emails send the right content to the right recipients.","title":"Catching emails in development"},{"location":"development/email/#using-helo","text":"Download and open the HELO app Launch studio, run start-all Install and activate postfix email backend: enable-email Configure postfix email backend to relay to EHLO app on Docker host machine: enable-email-relay host.docker.internal 2525 studio Or, using mailtrap: enable-email-relay smtp.mailtrap.io 2525 mailtrapusername mailtrappassword","title":"Using HELO"},{"location":"development/email/#sending-a-test-email","text":"From the studio: Install netcat: hab pkg install --binlink core/netcat Open SMTP connection: nc localhost 25 Start SMTP session: EHLO localhost.localdomain Set sender: MAIL FROM: <sender@example.com> Set recipient: RCPT TO: <recipient@example.com> Set message: DATA Subject: Hello world! This is the body of my email. Have a good day. . Close SMTP session: QUIT Review postfix backend log: less -S /hab/cache/sys.log","title":"Sending a test email"},{"location":"development/migrations/","text":"Developing Migrations \u00b6 Within the development studio: Create a new file under php-migrations/ Load modified working tree into runtime: update-site Execute all migrations: console-run migrations:execute --all (Re)Execute a specific migration: console-run migrations:execute --force \"Emergence/People/20191209_system-user\"","title":"Developing Migrations"},{"location":"development/migrations/#developing-migrations","text":"Within the development studio: Create a new file under php-migrations/ Load modified working tree into runtime: update-site Execute all migrations: console-run migrations:execute --all (Re)Execute a specific migration: console-run migrations:execute --force \"Emergence/People/20191209_system-user\"","title":"Developing Migrations"},{"location":"development/filesystem-api/","text":"Filesystem API \u00b6 This section documents the emergence-site-v1 filesystem API. The filesystem API consists of a declared set of standard root filesystem tree names for a site. Each standard root is associated with a definition of how files within should be named and structured, and how the site should incorporate the contents of the tree into its behavior.","title":"Filesystem API"},{"location":"development/filesystem-api/#filesystem-api","text":"This section documents the emergence-site-v1 filesystem API. The filesystem API consists of a declared set of standard root filesystem tree names for a site. Each standard root is associated with a definition of how files within should be named and structured, and how the site should incorporate the contents of the tree into its behavior.","title":"Filesystem API"},{"location":"development/filesystem-api/api-docs/","text":"api-docs \u00b6 The api-docs root serves to render a complete OpenAPI specification for the site. Within, the components of an OpenAPI specification are spread out over a tree structure to best enable overlaying projects to precisely override/extend API documentation.","title":"api-docs"},{"location":"development/filesystem-api/api-docs/#api-docs","text":"The api-docs root serves to render a complete OpenAPI specification for the site. Within, the components of an OpenAPI specification are spread out over a tree structure to best enable overlaying projects to precisely override/extend API documentation.","title":"api-docs"},{"location":"development/filesystem-api/console-commands/","text":"console-commands \u00b6 The console-commands root provides command-line console commands that developers can run in the context of the site. These can also be useful in DevOps automations that use shell scripting to orchestrate operations and to provide integration points for external shell-based systems.","title":"console-commands"},{"location":"development/filesystem-api/console-commands/#console-commands","text":"The console-commands root provides command-line console commands that developers can run in the context of the site. These can also be useful in DevOps automations that use shell scripting to orchestrate operations and to provide integration points for external shell-based systems.","title":"console-commands"},{"location":"development/filesystem-api/cypress/","text":"cypress \u00b6 The cypress root contains end-to-end testing assets for the Cypress browser testing framework . It follows the same structure that Cypress\u2019 command-line tooling will generate in a project repository by default. See Cypress\u2019 Folder Structure documentation for details on the semantics of content within this tree.","title":"cypress"},{"location":"development/filesystem-api/cypress/#cypress","text":"The cypress root contains end-to-end testing assets for the Cypress browser testing framework . It follows the same structure that Cypress\u2019 command-line tooling will generate in a project repository by default. See Cypress\u2019 Folder Structure documentation for details on the semantics of content within this tree.","title":"cypress"},{"location":"development/filesystem-api/data-exporters/","text":"data-exporters \u00b6 The data-exporters tree provides endpoints for querying potentially-dynamic data from the site and processing it record-by-record. The /exports web interface provides a menu of available data exporters with query forms and output to streaming CSV. Data warehouse exporters are available as well that can declaratively map output sets and attributes to external PostgreSQL tables and columns.","title":"data-exporters"},{"location":"development/filesystem-api/data-exporters/#data-exporters","text":"The data-exporters tree provides endpoints for querying potentially-dynamic data from the site and processing it record-by-record. The /exports web interface provides a menu of available data exporters with query forms and output to streaming CSV. Data warehouse exporters are available as well that can declaratively map output sets and attributes to external PostgreSQL tables and columns.","title":"data-exporters"},{"location":"development/filesystem-api/docs/","text":"docs \u00b6 The docs root contains documentation content for the project in Markdown format, commonly compiled with MkDocs into a static website. Sites can add files to this tree with any path and name to create arbitrary articles, but the following convention is recommended for typical documentation: getting-started/ : initial setup for a new system usage/ : content for users of the system development/ : content for developers making changes to the system operations/ : content for systems administrators maintaining the system","title":"docs"},{"location":"development/filesystem-api/docs/#docs","text":"The docs root contains documentation content for the project in Markdown format, commonly compiled with MkDocs into a static website. Sites can add files to this tree with any path and name to create arbitrary articles, but the following convention is recommended for typical documentation: getting-started/ : initial setup for a new system usage/ : content for users of the system development/ : content for developers making changes to the system operations/ : content for systems administrators maintaining the system","title":"docs"},{"location":"development/filesystem-api/dwoo-plugins/","text":"dwoo-plugins \u00b6 The dwoo-plugins root provides a way to extend the Dwoo templating language with new functions/plugins that can be called from within any template. Placing a file under this directory is enough to register a new plugin and make it usable, with the filename serving as the function name you can invoke the plugin with inside templates.","title":"dwoo-plugins"},{"location":"development/filesystem-api/dwoo-plugins/#dwoo-plugins","text":"The dwoo-plugins root provides a way to extend the Dwoo templating language with new functions/plugins that can be called from within any template. Placing a file under this directory is enough to register a new plugin and make it usable, with the filename serving as the function name you can invoke the plugin with inside templates.","title":"dwoo-plugins"},{"location":"development/filesystem-api/event-handlers/","text":"event-handlers \u00b6","title":"event-handlers"},{"location":"development/filesystem-api/event-handlers/#event-handlers","text":"","title":"event-handlers"},{"location":"development/filesystem-api/fixtures/","text":"","title":"Fixtures"},{"location":"development/filesystem-api/helm-chart/","text":"","title":"Helm chart"},{"location":"development/filesystem-api/html-templates/","text":"","title":"Html templates"},{"location":"development/filesystem-api/php-classes/","text":"","title":"Php classes"},{"location":"development/filesystem-api/php-config/","text":"","title":"Php config"},{"location":"development/filesystem-api/php-migrations/","text":"","title":"Php migrations"},{"location":"development/filesystem-api/phpunit-tests/","text":"","title":"Phpunit tests"},{"location":"development/filesystem-api/sencha-workspace/","text":"","title":"Sencha workspace"},{"location":"development/filesystem-api/site-root/","text":"","title":"Site root"},{"location":"development/filesystem-api/site-tasks/","text":"","title":"Site tasks"},{"location":"development/running-tests/e2e/","text":"End-to-end (E2E) testing \u00b6 Cypress is used to provide browser-level full-stack testing. In this project, Cypress gets run within the cypress-workspace holobranch defined at .holo/branch/cypress-workspace/** within the project repository. This allows local test suite additions and overrides to be stacked on top of those provided by parent projects. The base implemenation is published from emergence-skeleton and your local project may have any number of parent projects stacked in between, so there can be many layers contributing to the below content structure. The cypress-workspace holobranch typically copies the following overrides from the local project repository: cypress.json : top-level project configuration for cypress cypress/integrations/**/*.js : additional or overridden test suites cypress/integrations/**/*.json : additional or overridden test suite configurations (provides some flexibility for test suites to support different downstream reskinnings without all their code needing to be duplicated and overridden) cypress/fixtures/** : static content test suites can make use of Less commonly, the following files might also be copied from the local project repository to override the Cypress setup in more depth: cypress/support/index.js : Cypress plugins and additional commands get loaded here for all test suites package.json / package-lock.json : Tracks the Cypress version and those of installed plugin packages Try to avoid having copies of these in local project repositories: cypress/support/commands.js : Base set of additional commands that test suites can rely on. Instead of overridding this file, add additional project-specific commands to some new files under cypress/support and override cypress/support/index.js to load them cypress/plugins/index.js : Base set of automatic environment setup logic Run tests quickly \u00b6 To quickly run the full test suite headlessly, run on the local terminal outside the studio in the root of your local project repository: script/test Run tests interactively \u00b6 To run tests with Cypress\u2019 interactive GUI open, run on the local terminal outside the studio in the root of your local project repository: script/test-interactive This script uses unionfs-fuse to set up a virtual directory mount on your workstation\u2019s filesystem to run Cypress out of. This union mount provides a live workspace where your local project workspace is merged on top of the base set of cypress-workspace content pulled from your parent project. This virtual directory mount gets set up at ${path_to_your_repo}.cypress-workspace/merged and Cypress gets run from there. Changes you save to Cypress content in your local project work tree will immediately be reflected in the merged mount The filesystem events needed to drive auto-reload may not work Exit the Cypress GUI and reload it to thoroughly force your latest content to be used Changes you save to Cypress content in your local merged mount will immediately be reflected back to your local project work tree If parent project content changes / you\u2019ve edited a source config, exit the Cypress GUI and re-run script/test-interactive to restart in a fresh environment Making Cypress auto-reload as you save changes Because filesystem change events from your local project work tree to the merged unionfs that Cypress runs out of don\u2019t always work, work on Cypress tests out of the merged mount instead. Any changes you make will immediately be written to to your local project work tree ready to stage into a git commit, and filesystem change events will fire live for Cypress to auto-reload test suites as you work. The Open in IDE button that Cypress\u2019 main window will how you as you hover over tests in the list can be used to open the copy of the file in the merged mount where changes will trigger auto-reload. Testing against a remote server \u00b6 By setting environment variables before launching the Cypress GUI, the E2E test suite can be configured to execute against a backend studio hosted on a remote machine or server. On the local terminal outside the studio in the root of your local project repository: Set base URL to studo HTTP root reachable from local workstation: export CYPRESS_BASE_URL = 'http://workstation.mydomain:9070' Configure the SSH host that the backend studio is running on: export CYPRESS_STUDIO_SSH = 'workstation.mydomain' Your local terminal must be set up to connect to it without password. Configure the name of the Docker container running the backend studio: export CYPRESS_STUDIO_CONTAINER = 'laddr-studio' Launch the Cypress GUI: script/test-interactive","title":"End-to-end (E2E) testing"},{"location":"development/running-tests/e2e/#end-to-end-e2e-testing","text":"Cypress is used to provide browser-level full-stack testing. In this project, Cypress gets run within the cypress-workspace holobranch defined at .holo/branch/cypress-workspace/** within the project repository. This allows local test suite additions and overrides to be stacked on top of those provided by parent projects. The base implemenation is published from emergence-skeleton and your local project may have any number of parent projects stacked in between, so there can be many layers contributing to the below content structure. The cypress-workspace holobranch typically copies the following overrides from the local project repository: cypress.json : top-level project configuration for cypress cypress/integrations/**/*.js : additional or overridden test suites cypress/integrations/**/*.json : additional or overridden test suite configurations (provides some flexibility for test suites to support different downstream reskinnings without all their code needing to be duplicated and overridden) cypress/fixtures/** : static content test suites can make use of Less commonly, the following files might also be copied from the local project repository to override the Cypress setup in more depth: cypress/support/index.js : Cypress plugins and additional commands get loaded here for all test suites package.json / package-lock.json : Tracks the Cypress version and those of installed plugin packages Try to avoid having copies of these in local project repositories: cypress/support/commands.js : Base set of additional commands that test suites can rely on. Instead of overridding this file, add additional project-specific commands to some new files under cypress/support and override cypress/support/index.js to load them cypress/plugins/index.js : Base set of automatic environment setup logic","title":"End-to-end (E2E) testing"},{"location":"development/running-tests/e2e/#run-tests-quickly","text":"To quickly run the full test suite headlessly, run on the local terminal outside the studio in the root of your local project repository: script/test","title":"Run tests quickly"},{"location":"development/running-tests/e2e/#run-tests-interactively","text":"To run tests with Cypress\u2019 interactive GUI open, run on the local terminal outside the studio in the root of your local project repository: script/test-interactive This script uses unionfs-fuse to set up a virtual directory mount on your workstation\u2019s filesystem to run Cypress out of. This union mount provides a live workspace where your local project workspace is merged on top of the base set of cypress-workspace content pulled from your parent project. This virtual directory mount gets set up at ${path_to_your_repo}.cypress-workspace/merged and Cypress gets run from there. Changes you save to Cypress content in your local project work tree will immediately be reflected in the merged mount The filesystem events needed to drive auto-reload may not work Exit the Cypress GUI and reload it to thoroughly force your latest content to be used Changes you save to Cypress content in your local merged mount will immediately be reflected back to your local project work tree If parent project content changes / you\u2019ve edited a source config, exit the Cypress GUI and re-run script/test-interactive to restart in a fresh environment Making Cypress auto-reload as you save changes Because filesystem change events from your local project work tree to the merged unionfs that Cypress runs out of don\u2019t always work, work on Cypress tests out of the merged mount instead. Any changes you make will immediately be written to to your local project work tree ready to stage into a git commit, and filesystem change events will fire live for Cypress to auto-reload test suites as you work. The Open in IDE button that Cypress\u2019 main window will how you as you hover over tests in the list can be used to open the copy of the file in the merged mount where changes will trigger auto-reload.","title":"Run tests interactively"},{"location":"development/running-tests/e2e/#testing-against-a-remote-server","text":"By setting environment variables before launching the Cypress GUI, the E2E test suite can be configured to execute against a backend studio hosted on a remote machine or server. On the local terminal outside the studio in the root of your local project repository: Set base URL to studo HTTP root reachable from local workstation: export CYPRESS_BASE_URL = 'http://workstation.mydomain:9070' Configure the SSH host that the backend studio is running on: export CYPRESS_STUDIO_SSH = 'workstation.mydomain' Your local terminal must be set up to connect to it without password. Configure the name of the Docker container running the backend studio: export CYPRESS_STUDIO_CONTAINER = 'laddr-studio' Launch the Cypress GUI: script/test-interactive","title":"Testing against a remote server"},{"location":"development/workspace-setup/content-editor/","text":"Content Editor webapp \u00b6 Code layout \u00b6 sencha-workspace/ packages/ emergence-cms/ : Primary location for content editor UI code emr-skeleton-theme/ : The Sencha theme used when rendering the content editor EmergenceContentEditor/ : A thin Sencha application used to make development easier and to provide a build target for generating the theme html-templates/ webapps/EmergenceContentEditor/sencha.tpl : A template for rendering the content editor embedded in the site\u2019s design frame html-templates/blog/blogPostEdit.tpl : A wrapper around sencha.tpl to provide the content editor UI on the blog post edit form html-templates/pages/pageEdit.tpl : A wrapper around sencha.tpl to provide the content editor UI on the page edit form Running live changes \u00b6 The frontend Sencha application needs to be built at least once with the Sencha CMD build tool to scaffold/update a set of loader files. After that, you can just edit files the working tree and reload the browser. The two exceptions where you need to build again are changing the list of packages or changing the list of override files. Before the frontend application can be built to run from live changes, you\u2019ll need to ensure all submodules are initialized: git submodule update --init Then, use the shortcut studio command for building the frontend application: build-content-editor Once built, the live-editable version of the app can be accessed via the static web server that the studio runs on port {{ no such element: dict object['static_port'] }} . The backend host must be provided to the apps via the ?apiHost query parameter. Any remote backend with CORS enabled will work, or you can use the local backend: localhost:{{ no such element: dict object['static_port'] }}/EmergenceContentEditor/?apiHost=localhost:9070 Working with breakpoints \u00b6 By default, the Sencha framework will automatically append random cache buster values to every loaded .js source. This helps ensures that your latest code always runs, but will also prevent any breakpoints you set from persisting across reloads. With the Disable cache option of the network inspector activated, you can disable this built-in cache buster by appending &cache=1 to the current page\u2019s query string. Connecting to remote server \u00b6 You can connect to any remote instance that has CORS enabled by appending the query parameter apiHost when loading the page. If the remote instance requires HTTPS, append apiSSL=1 as well.","title":"Content Editor webapp"},{"location":"development/workspace-setup/content-editor/#content-editor-webapp","text":"","title":"Content Editor webapp"},{"location":"development/workspace-setup/content-editor/#code-layout","text":"sencha-workspace/ packages/ emergence-cms/ : Primary location for content editor UI code emr-skeleton-theme/ : The Sencha theme used when rendering the content editor EmergenceContentEditor/ : A thin Sencha application used to make development easier and to provide a build target for generating the theme html-templates/ webapps/EmergenceContentEditor/sencha.tpl : A template for rendering the content editor embedded in the site\u2019s design frame html-templates/blog/blogPostEdit.tpl : A wrapper around sencha.tpl to provide the content editor UI on the blog post edit form html-templates/pages/pageEdit.tpl : A wrapper around sencha.tpl to provide the content editor UI on the page edit form","title":"Code layout"},{"location":"development/workspace-setup/content-editor/#running-live-changes","text":"The frontend Sencha application needs to be built at least once with the Sencha CMD build tool to scaffold/update a set of loader files. After that, you can just edit files the working tree and reload the browser. The two exceptions where you need to build again are changing the list of packages or changing the list of override files. Before the frontend application can be built to run from live changes, you\u2019ll need to ensure all submodules are initialized: git submodule update --init Then, use the shortcut studio command for building the frontend application: build-content-editor Once built, the live-editable version of the app can be accessed via the static web server that the studio runs on port {{ no such element: dict object['static_port'] }} . The backend host must be provided to the apps via the ?apiHost query parameter. Any remote backend with CORS enabled will work, or you can use the local backend: localhost:{{ no such element: dict object['static_port'] }}/EmergenceContentEditor/?apiHost=localhost:9070","title":"Running live changes"},{"location":"development/workspace-setup/content-editor/#working-with-breakpoints","text":"By default, the Sencha framework will automatically append random cache buster values to every loaded .js source. This helps ensures that your latest code always runs, but will also prevent any breakpoints you set from persisting across reloads. With the Disable cache option of the network inspector activated, you can disable this built-in cache buster by appending &cache=1 to the current page\u2019s query string.","title":"Working with breakpoints"},{"location":"development/workspace-setup/content-editor/#connecting-to-remote-server","text":"You can connect to any remote instance that has CORS enabled by appending the query parameter apiHost when loading the page. If the remote instance requires HTTPS, append apiSSL=1 as well.","title":"Connecting to remote server"},{"location":"development/workspace-setup/local-studio/","text":"Local Studio Container \u00b6 This guide will walk you through launching a Docker-container local development studio and using it to test changes made within a local Git repository. Launch studio container \u00b6 Install Chef Habitat: curl -s https://raw.githubusercontent.com/habitat-sh/habitat/master/components/hab/install.sh | sudo bash Set up Chef Habitat, accepting defaults for all prompts: hab setup Clone laddr repository and any submodules: git clone --recursive git@github.com:CodeForPhilly/laddr.git Change into cloned directory: cd ./laddr Launch studio: Use the included scripts-to-rules-them-all workflow script to configure and launch a studio session: script/studio Review the notes printed to your terminal at the end of the studio startup process for a list of all available studio commands. Bootstrap and develop backend \u00b6 Start services: Use the studio command start-all to launch the http server (nginx), the application runtime (php-fpm), and a local mysql server: start-all At this point, you should be able to open localhost:9070 and see the error message Page not found . Build site: To build the entire site and load it, use the studio command update-site : update-site At this point, localhost:9070 should display the current build of the site Load fixture data into site database (optional): load-fixtures The standard fixture data includes the following users: Username Password AccountLevel About system system Developer Full system access admin admin Administrator Manage site and staff staff staff Staff Staff access user user User Regular user Make and apply changes: After editing code in the working tree, you must rebuild and update the site: update-site A command to automatically rebuild and update the site as changes are made to the working tree is also available, but currently not that efficient or reliable: watch-site Enable user registration \u00b6 To enable user registration on a site that comes with it disabled: # write class configuring enabling registration mkdir -p php-config/Emergence/People echo '<?php Emergence\\People\\RegistrationRequestHandler::$enableRegistration = true;' > php-config/Emergence/People/RegistrationRequestHandler.config.php # rebuild environment update-site After visiting /register and creating a new user account, you can use the studio command promote-user to upgrade the user account you just registered to the highest access level: promote-user <myuser> Connect to local database \u00b6 The studio container hosts a local MySQL instance that can be connected to at: Host : localhost (or LAN/WAN IP of machine hosting Docker engine) Port : 9076 Username : admin Password : admin","title":"Local Studio Container"},{"location":"development/workspace-setup/local-studio/#local-studio-container","text":"This guide will walk you through launching a Docker-container local development studio and using it to test changes made within a local Git repository.","title":"Local Studio Container"},{"location":"development/workspace-setup/local-studio/#launch-studio-container","text":"Install Chef Habitat: curl -s https://raw.githubusercontent.com/habitat-sh/habitat/master/components/hab/install.sh | sudo bash Set up Chef Habitat, accepting defaults for all prompts: hab setup Clone laddr repository and any submodules: git clone --recursive git@github.com:CodeForPhilly/laddr.git Change into cloned directory: cd ./laddr Launch studio: Use the included scripts-to-rules-them-all workflow script to configure and launch a studio session: script/studio Review the notes printed to your terminal at the end of the studio startup process for a list of all available studio commands.","title":"Launch studio container"},{"location":"development/workspace-setup/local-studio/#bootstrap-and-develop-backend","text":"Start services: Use the studio command start-all to launch the http server (nginx), the application runtime (php-fpm), and a local mysql server: start-all At this point, you should be able to open localhost:9070 and see the error message Page not found . Build site: To build the entire site and load it, use the studio command update-site : update-site At this point, localhost:9070 should display the current build of the site Load fixture data into site database (optional): load-fixtures The standard fixture data includes the following users: Username Password AccountLevel About system system Developer Full system access admin admin Administrator Manage site and staff staff staff Staff Staff access user user User Regular user Make and apply changes: After editing code in the working tree, you must rebuild and update the site: update-site A command to automatically rebuild and update the site as changes are made to the working tree is also available, but currently not that efficient or reliable: watch-site","title":"Bootstrap and develop backend"},{"location":"development/workspace-setup/local-studio/#enable-user-registration","text":"To enable user registration on a site that comes with it disabled: # write class configuring enabling registration mkdir -p php-config/Emergence/People echo '<?php Emergence\\People\\RegistrationRequestHandler::$enableRegistration = true;' > php-config/Emergence/People/RegistrationRequestHandler.config.php # rebuild environment update-site After visiting /register and creating a new user account, you can use the studio command promote-user to upgrade the user account you just registered to the highest access level: promote-user <myuser>","title":"Enable user registration"},{"location":"development/workspace-setup/local-studio/#connect-to-local-database","text":"The studio container hosts a local MySQL instance that can be connected to at: Host : localhost (or LAN/WAN IP of machine hosting Docker engine) Port : 9076 Username : admin Password : admin","title":"Connect to local database"},{"location":"development/workspace-setup/virtual-multisite/","text":"Virtual Multi-site Container \u00b6 Launch virtual multisite container \u00b6 docker run \\ --name emergence \\ -v emergence:/emergence \\ -p 80 :80 \\ -p 3306 :3306 \\ -p 9083 :9083 \\ jarvus/emergence","title":"Virtual Multi-site Container"},{"location":"development/workspace-setup/virtual-multisite/#virtual-multi-site-container","text":"","title":"Virtual Multi-site Container"},{"location":"development/workspace-setup/virtual-multisite/#launch-virtual-multisite-container","text":"docker run \\ --name emergence \\ -v emergence:/emergence \\ -p 80 :80 \\ -p 3306 :3306 \\ -p 9083 :9083 \\ jarvus/emergence","title":"Launch virtual multisite container"},{"location":"getting-started/","text":"Getting Started \u00b6 The Getting Started section provides content covering: Overviews of the system Obtaining a running instance of the system Configuring a new system for an organization Onboarding new users into the system","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"The Getting Started section provides content covering: Overviews of the system Obtaining a running instance of the system Configuring a new system for an organization Onboarding new users into the system","title":"Getting Started"},{"location":"operations/","text":"Operations \u00b6 The Operations section provides content covering: Overviews of the physical infrastructure and service components Building hosted environments Maintaining hosted environments Backing up and restoring content Monitoring system health","title":"Operations"},{"location":"operations/#operations","text":"The Operations section provides content covering: Overviews of the physical infrastructure and service components Building hosted environments Maintaining hosted environments Backing up and restoring content Monitoring system health","title":"Operations"},{"location":"operations/update-saml2-certificate/","text":"Update SAML2 Certificate \u00b6 The OpenSSL certificate used by Laddr\u2019s Single Sign-On (SSO) integration with Slack needs to be refreshed occasionally when it nears or passes its expiration date Generate a new certificate \u00b6 On any computer with the openssl command installed (readily available on macOS and Linux), you can generate the new key+certificate pair before installing it to your Slack and Laddr instances: Generate private key: openssl genrsa \\ -out ./laddr-slack-private-key.pem \\ 1024 Generate public certificate: openssl req -new -x509 \\ -days 1095 \\ -key ./laddr-slack-private-key.pem \\ -out ./laddr-slack-public-cert.pem Fill out the prompts with appropriate information about your organization. These values don\u2019t really matter for anything If your Laddr instance is hosted on Kubernetes, encode the two generated files into a Secret manifest (you only need the kubectl command installed on your local system for this, it does not need to be connected to any cluster): kubectl create secret generic saml2 \\ --output = yaml \\ --dry-run \\ --from-file = SAML2_PRIVATE_KEY = ./laddr-slack-private-key.pem \\ --from-file = SAML2_CERTIFICATE = ./laddr-slack-public-cert.pem \\ > ./saml2.secret.yaml If your cluster uses sealed secrets , seal the newly-created secret: export SEALED_SECRETS_CERT = https://sealed-secrets.live.k8s.phl.io/v1/cert.pem kubeseal \\ --namespace \"my-project\" \\ -f ./saml2.secret.yaml \\ -w ./saml2.sealed-secret.yaml Be sure to replace my-project with the namespace your instance is deployed within Deploy the sealed secret to your cluster In Code for Philly\u2019s case, that means updating saml2.yaml with the new content and then merging the generated deploy PR. After the deploy, you may need to delete the existing secret in order for the sealed-secrets operator to replace it with the updated secret Finally, visit https://my-org.slack.com/admin/auth/saml?sudo=1 and edit the Public Certificate , pasting the contents of ./laddr-slack-public-cert.pem : cat ./laddr-slack-public-cert.pem # paste output to Slack admin webpage Slack will not let you save the new public certificate until it\u2019s been successfully applied to the host","title":"Update SAML2 Certificate"},{"location":"operations/update-saml2-certificate/#update-saml2-certificate","text":"The OpenSSL certificate used by Laddr\u2019s Single Sign-On (SSO) integration with Slack needs to be refreshed occasionally when it nears or passes its expiration date","title":"Update SAML2 Certificate"},{"location":"operations/update-saml2-certificate/#generate-a-new-certificate","text":"On any computer with the openssl command installed (readily available on macOS and Linux), you can generate the new key+certificate pair before installing it to your Slack and Laddr instances: Generate private key: openssl genrsa \\ -out ./laddr-slack-private-key.pem \\ 1024 Generate public certificate: openssl req -new -x509 \\ -days 1095 \\ -key ./laddr-slack-private-key.pem \\ -out ./laddr-slack-public-cert.pem Fill out the prompts with appropriate information about your organization. These values don\u2019t really matter for anything If your Laddr instance is hosted on Kubernetes, encode the two generated files into a Secret manifest (you only need the kubectl command installed on your local system for this, it does not need to be connected to any cluster): kubectl create secret generic saml2 \\ --output = yaml \\ --dry-run \\ --from-file = SAML2_PRIVATE_KEY = ./laddr-slack-private-key.pem \\ --from-file = SAML2_CERTIFICATE = ./laddr-slack-public-cert.pem \\ > ./saml2.secret.yaml If your cluster uses sealed secrets , seal the newly-created secret: export SEALED_SECRETS_CERT = https://sealed-secrets.live.k8s.phl.io/v1/cert.pem kubeseal \\ --namespace \"my-project\" \\ -f ./saml2.secret.yaml \\ -w ./saml2.sealed-secret.yaml Be sure to replace my-project with the namespace your instance is deployed within Deploy the sealed secret to your cluster In Code for Philly\u2019s case, that means updating saml2.yaml with the new content and then merging the generated deploy PR. After the deploy, you may need to delete the existing secret in order for the sealed-secrets operator to replace it with the updated secret Finally, visit https://my-org.slack.com/admin/auth/saml?sudo=1 and edit the Public Certificate , pasting the contents of ./laddr-slack-public-cert.pem : cat ./laddr-slack-public-cert.pem # paste output to Slack admin webpage Slack will not let you save the new public certificate until it\u2019s been successfully applied to the host","title":"Generate a new certificate"},{"location":"usage/","text":"Usage \u00b6 The Usage section provides content covering: Overviews of the system features Setting up and customizing a user account Using the system\u2019s features for day-to-day work Reporting and integration","title":"Usage"},{"location":"usage/#usage","text":"The Usage section provides content covering: Overviews of the system features Setting up and customizing a user account Using the system\u2019s features for day-to-day work Reporting and integration","title":"Usage"}]}