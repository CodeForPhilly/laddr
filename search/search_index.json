{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>Welcome, browse sections at the top of the site.</p>"},{"location":"#roadmap","title":"Roadmap","text":"<ul> <li> Merge content from http://emergenceplatform.github.io/book/</li> <li> Merge content from http://slate.is/docs</li> <li> Merge content from http://forum.laddr.us/</li> <li> Merge content from http://forum.emr.ge/</li> <li> Merge content from https://github.com/JarvusInnovations/emergence-docs (most dated)</li> </ul>"},{"location":"development/","title":"Development","text":"<p>The Development section provides content covering:</p> <ul> <li>Overviews of the internal architecture and components</li> <li>Obtaining development environments</li> <li>Executing development workflows</li> <li>Feature implementation guides and examples</li> </ul>"},{"location":"development/clone-from-git/","title":"Clone Laddr from git","text":"<p>This guide is for developers who want to work on Laddr\u2019s core code. It will walk you through setting up a fresh site instance and cloning a version of Laddr into it from a remote git repository.</p>"},{"location":"development/clone-from-git/#step-1-obtain-an-emergence-host","title":"Step 1: Obtain an emergence host","text":"<p>You will need a host server dedicated to running emergence. If you don\u2019t have access to one already, the easiest way to get started is to spin up a small Ubuntu 16.04 LTS virtual machine with a cloud provider like Digital Ocean, Google Cloud Compute, AWS, or countless others. Once you are logged in to your fresh Ubuntu 16.04 machine, follow emergence\u2019s installation guide to prepare it for hosting emergence-powered sites like Laddr.</p> <p>Alternatively, if you\u2019re familiar with Docker, you can spin up an emergence container:</p> <pre><code>docker run -d \\\n-it \\\n--name emergence \\\n-v /emergence:/emergence \\\n-p 127.0.0.10:80:80 \\\n-p 127.0.0.10:3306:3306 \\\n-p 127.0.0.10:9083:9083 \\\njarvus/emergence \\\ntmux new -s emergence emergence-kernel\n</code></pre>"},{"location":"development/clone-from-git/#step-2-create-a-site-for-your-laddr-development-instance","title":"Step 2: Create a site for your laddr development instance","text":"<p>Laddr is based on emergence\u2019s <code>skeleton-v2</code> site template. Unlike when provising a deployment instance of Laddr, for development you want to create a site extending Laddr\u2019s parent site like Laddr does rather than Laddr itself. Laddr\u2019s code will be cloned from git and applied on top of the parent site.</p> <p>Use emergence\u2019s host control panel to create a new site with your desired hostname and initial user, just be sure to select <code>skeleton-v2.emr.ge</code> as the parent hostname. After the site is created login to /develop with your initial user developer account.</p>"},{"location":"development/clone-from-git/#step-3-configure-mapping-to-the-laddr-git-repository","title":"Step 3: Configure mapping to the laddr git repository","text":"<p>To configure a link between your emergence instance and a git repository, create a new file at <code>php-config/Git.config.d/laddr.php</code> and copy its initial contents from the the file at the same path in Laddr\u2019s develop branch on Github.</p> <p>Optionally, edit the <code>remote</code> option to point at your own fork, and switch it to the SSH protocol if you\u2019d like to be able to push changes from the web UI.</p>"},{"location":"development/clone-from-git/#step-4-initialize-git-repository","title":"Step 4: Initialize git repository","text":"<p>Visit /site-admin/sources to initialize the configured git repository. If you switch the remote to an SSH git URL before initializing, a deploy key will be generated for you that you can install on GitHub before continueing to enable web-based read/write access. Otherwise, if you are cloning via HTTPS, you will need to SSH into the server and use the git CLI to push changes after initializing the repository.</p>"},{"location":"development/clone-from-git/#step-5-pull-code-from-git","title":"Step 5: Pull code from git","text":"<p>Visit /site-admin/sources/laddr and click Pull if needed to pull the latest commits from github into your git working copy. Then click the Sync -&gt; Update emergence VFS button to import the git working tree copy into your emergence instance.</p>"},{"location":"development/clone-from-git/#next-steps","title":"Next steps","text":"<p>If you run into any trouble, need to reconfigure the repository, or execute any advanced maneuvers, use <code>emergence-git-shell my-instance-name laddr</code> on your host machine to drop into a properly-configured git shell where you can make full use of the git CLI client without any permissions issues.</p>"},{"location":"development/email/","title":"Working with Email","text":""},{"location":"development/email/#catching-emails-in-development","title":"Catching emails in development","text":"<p>In the studio, you can enable relaying email to a specified SMTP endpoint.</p> <p>HELO and mailtrap are easy SMTP endpoints to set up for development that provide UIs for reviewing emails sent to all recipients. When emails are configured to be relayed to these services, they will be trapped for review and never actually delivered to anyone externally, no matter what recipients you use. As opposed to overriding recipient emails to test email features, this approach enables you to verify that personalized bulk emails send the right content to the right recipients.</p>"},{"location":"development/email/#using-helo","title":"Using HELO","text":"<ol> <li>Download and open the HELO app</li> <li>Launch studio, run <code>start-all</code></li> <li> <p>Install and activate postfix email backend:</p> <pre><code>enable-email\n</code></pre> </li> <li> <p>Configure postfix email backend to relay to HELO app on Docker host machine:</p> <pre><code>enable-email-relay host.docker.internal 2525 studio\n</code></pre> <p>Or, using mailtrap:</p> <pre><code>enable-email-relay smtp.mailtrap.io 2525 mailtrapusername mailtrappassword\n</code></pre> </li> </ol>"},{"location":"development/email/#sending-a-test-email","title":"Sending a test email","text":"<p>From the studio:</p> <ol> <li> <p>Install netcat:</p> <pre><code>hab pkg install --binlink core/netcat\n</code></pre> </li> <li> <p>Open SMTP connection:</p> <pre><code>nc localhost 25\n</code></pre> </li> <li> <p>Start SMTP session:</p> <pre><code>EHLO localhost.localdomain\n</code></pre> </li> <li> <p>Set sender:</p> <pre><code>MAIL FROM: &lt;sender@example.com&gt;\n</code></pre> </li> <li> <p>Set recipient:</p> <pre><code>RCPT TO: &lt;recipient@example.com&gt;\n</code></pre> </li> <li> <p>Set message:</p> <pre><code>DATA\nSubject: Hello world!\n\nThis is the body of my email.\n\nHave a good day.\n\n.\n</code></pre> </li> <li> <p>Close SMTP session:</p> <pre><code>QUIT\n</code></pre> </li> <li> <p>Review postfix backend log:</p> <pre><code>less -S /hab/cache/sys.log\n</code></pre> </li> </ol>"},{"location":"development/migrations/","title":"Developing Migrations","text":"<p>Within the development studio:</p> <ol> <li>Create a new file under <code>php-migrations/</code></li> <li> <p>Load modified working tree into runtime:</p> <pre><code>update-site\n</code></pre> </li> <li> <p>Execute all migrations:</p> <pre><code>console-run migrations:execute --all\n</code></pre> </li> <li> <p>(Re)Execute a specific migration:</p> <pre><code>console-run migrations:execute --force \"Emergence/People/20191209_system-user\"\n</code></pre> </li> </ol>"},{"location":"development/filesystem-api/","title":"Filesystem API","text":"<p>This section documents the <code>emergence-site-v1</code> filesystem API.</p> <p>The filesystem API consists of a declared set of standard root filesystem tree names for a site. Each standard root is associated with a definition of how files within should be named and structured, and how the site should incorporate the contents of the tree into its behavior.</p>"},{"location":"development/filesystem-api/api-docs/","title":"api-docs","text":"<p>The <code>api-docs</code> root serves to render a complete OpenAPI specification for the site. Within, the components of an OpenAPI specification are spread out over a tree structure to best enable overlaying projects to precisely override/extend API documentation.</p>"},{"location":"development/filesystem-api/console-commands/","title":"console-commands","text":"<p>The <code>console-commands</code> root provides command-line console commands that developers can run in the context of the site. These can also be useful in DevOps automations that use shell scripting to orchestrate operations and to provide integration points for external shell-based systems.</p>"},{"location":"development/filesystem-api/cypress/","title":"cypress","text":"<p>The <code>cypress</code> root contains end-to-end testing assets for the Cypress browser testing framework. It follows the same structure that Cypress\u2019 command-line tooling will generate in a project repository by default.</p> <p>See Cypress\u2019 Folder Structure documentation for details on the semantics of content within this tree.</p>"},{"location":"development/filesystem-api/data-exporters/","title":"data-exporters","text":"<p>The <code>data-exporters</code> tree provides endpoints for querying potentially-dynamic data from the site and processing it record-by-record.</p> <p>The <code>/exports</code> web interface provides a menu of available data exporters with query forms and output to streaming CSV.</p> <p>Data warehouse exporters are available as well that can declaratively map output sets and attributes to external PostgreSQL tables and columns.</p>"},{"location":"development/filesystem-api/docs/","title":"docs","text":"<p>The <code>docs</code> root contains documentation content for the project in Markdown format, commonly compiled with MkDocs into a static website.</p> <p>Sites can add files to this tree with any path and name to create arbitrary articles, but the following convention is recommended for typical documentation:</p> <ul> <li><code>getting-started/</code>: initial setup for a new system</li> <li><code>usage/</code>: content for users of the system</li> <li><code>development/</code>: content for developers making changes to the system</li> <li><code>operations/</code>: content for systems administrators maintaining the system</li> </ul>"},{"location":"development/filesystem-api/dwoo-plugins/","title":"dwoo-plugins","text":"<p>The <code>dwoo-plugins</code> root provides a way to extend the Dwoo templating language with new functions/plugins that can be called from within any template.</p> <p>Placing a file under this directory is enough to register a new plugin and make it usable, with the filename serving as the function name you can invoke the plugin with inside templates.</p>"},{"location":"development/filesystem-api/event-handlers/","title":"event-handlers","text":""},{"location":"development/running-tests/e2e/","title":"End-to-end (E2E) testing","text":"<p>Cypress is used to provide browser-level full-stack testing.</p> <p>In this project, Cypress gets run within the <code>cypress-workspace</code> holobranch defined at <code>.holo/branch/cypress-workspace/**</code> within the project repository. This allows local test suite additions and overrides to be stacked on top of those provided by parent projects. The base implemenation is published from <code>emergence-skeleton</code> and your local project may have any number of parent projects stacked in between, so there can be many layers contributing to the below content structure.</p> <p>The <code>cypress-workspace</code> holobranch typically copies the following overrides from the local project repository:</p> <ul> <li><code>cypress.json</code>: top-level project configuration for cypress</li> <li><code>cypress/integrations/**/*.js</code>: additional or overridden test suites</li> <li><code>cypress/integrations/**/*.json</code>: additional or overridden test suite configurations (provides some flexibility for test suites to support different downstream reskinnings without all their code needing to be duplicated and overridden)</li> <li><code>cypress/fixtures/**</code>: static content test suites can make use of</li> </ul> <p>Less commonly, the following files might also be copied from the local project repository to override the Cypress setup in more depth:</p> <ul> <li><code>cypress/support/index.js</code>: Cypress plugins and additional commands get loaded here for all test suites</li> <li><code>package.json</code>/<code>package-lock.json</code>: Tracks the Cypress version and those of installed plugin packages</li> </ul> <p>Try to avoid having copies of these in local project repositories:</p> <ul> <li><code>cypress/support/commands.js</code>: Base set of additional commands that test suites can rely on. Instead of overridding this file, add additional project-specific commands to some new files under <code>cypress/support</code> and override <code>cypress/support/index.js</code> to load them</li> <li><code>cypress/plugins/index.js</code>: Base set of automatic environment setup logic</li> </ul>"},{"location":"development/running-tests/e2e/#run-tests-quickly","title":"Run tests quickly","text":"<p>To quickly run the full test suite headlessly, run on the local terminal outside the studio in the root of your local project repository:</p> <pre><code>script/test\n</code></pre>"},{"location":"development/running-tests/e2e/#run-tests-interactively","title":"Run tests interactively","text":"<p>To run tests with Cypress\u2019 interactive GUI open, run on the local terminal outside the studio in the root of your local project repository:</p> <pre><code>script/test-interactive\n</code></pre> <p>This script uses <code>unionfs-fuse</code> to set up a virtual directory mount on your workstation\u2019s filesystem to run Cypress out of. This union mount provides a live workspace where your local project workspace is merged on top of the base set of <code>cypress-workspace</code> content pulled from your parent project.</p> <p>This virtual directory mount gets set up at <code>${path_to_your_repo}.cypress-workspace/merged</code> and Cypress gets run from there.</p> <ul> <li>Changes you save to Cypress content in your local project work tree will immediately be reflected in the <code>merged</code> mount<ul> <li>The filesystem events needed to drive auto-reload may not work</li> <li>Exit the Cypress GUI and reload it to thoroughly force your latest content to be used</li> </ul> </li> <li>Changes you save to Cypress content in your local <code>merged</code> mount will immediately be reflected back to your local project work tree</li> <li>If parent project content changes / you\u2019ve edited a source config, exit the Cypress GUI and re-run <code>script/test-interactive</code> to restart in a fresh environment</li> </ul> <p>Making Cypress auto-reload as you save changes</p> <p>Because filesystem change events from your local project work tree to the merged unionfs that Cypress runs out of don\u2019t always work, work on Cypress tests out of the <code>merged</code> mount instead.</p> <p>Any changes you make will immediately be written to to your local project work tree ready to stage into a git commit, and filesystem change events will fire live for Cypress to auto-reload test suites as you work.</p> <p>The Open in IDE button that Cypress\u2019 main window will how you as you hover over tests in the list can be used to open the copy of the file in the <code>merged</code> mount where changes will trigger auto-reload.</p> <p>Prevent VSCode from opening virtual repository</p> <p>By default, Visual Studio Code will automatically detect and open the \u201cmerged\u201d git repository produced by the unionfs with its built-in git integration. This makes it difficult to close out the testing environment as VSCode will keep many active processes accessing the git repository once it has been opened, even after you manually close it.</p> <p>To prevent Visual Studio Code from automatically opening this union repository and causing all sorts of mahem, open your user <code>settings.json</code> and add an option to ignore the <code>*.cypress-workspace/merged</code> repository at whatever path your project repository lives at:</p> <pre><code>{\n// ...\n\"git.ignoredRepositories\": [\n\"/Users/me/Repositories/git@github.com:CodeForPhilly/laddr.git.cypress-workspace/merged\"\n]\n}\n</code></pre>"},{"location":"development/running-tests/e2e/#testing-against-a-remote-server","title":"Testing against a remote server","text":"<p>By setting environment variables before launching the Cypress GUI, the E2E test suite can be configured to execute against a backend studio hosted on a remote machine or server.</p> <p>On the local terminal outside the studio in the root of your local project repository:</p> <ol> <li> <p>Set base URL to studo HTTP root reachable from local workstation:</p> <pre><code>export CYPRESS_BASE_URL='http://workstation.mydomain:9070'\n</code></pre> </li> <li> <p>Configure the SSH host that the backend studio is running on:</p> <pre><code>export CYPRESS_STUDIO_SSH='workstation.mydomain'\n</code></pre> <p>Your local terminal must be set up to connect to it without password.</p> </li> <li> <p>Configure the name of the Docker container running the backend studio:</p> <pre><code>export CYPRESS_STUDIO_CONTAINER='laddr-studio'\n</code></pre> </li> <li> <p>Launch the Cypress GUI:</p> <pre><code>script/test-interactive\n</code></pre> </li> </ol>"},{"location":"development/running-tests/e2e/#checking-for-race-conditions","title":"Checking for race conditions","text":"<p>Cypress\u2019 default timeout limits for UI assertions and XHR assertions vary greatly: 4 seconds for UI assertions and 30 seconds for XHR assertions. This disparity is a leading cause of apparent instability in Cypress tests where a test will pass locally consistently but fail in CI consistency.</p> <p>What causes this to happen is having UI assertions within your tests that won\u2019t pass if some XHR call leading up to them takes more than 4 seconds, which is more likely to happen occasionally on \u201ccold\u201d instances like CI runs always are. The solution to this is relatively simple: add a <code>cy.wait('@xhrInterceptName')</code> assertion ahead of any UI assertion that won\u2019t pass until that XHR call finishes. This ensures that the test script waits for XHR calls to finish under the longer timeout they have by default, and gives you more helpful errors when a failure happens at that level.</p> <p>It can be easy to miss these spots while you\u2019re developing tests. One step is to look through your test logs for all XHR calls (they\u2019ll show up in the actions log timeline whether you intercept or wait for them or not) and consider if any of the UI assertions surrounding it depend on it finishing first. Another approach is to introduce an artificial delay to all XHR calls on your local server, forcing them all to take longer than the default UI assertion timeout of 4 seconds:</p> <pre><code>&lt;?php\n// save to php-config/Site.config.d/delay-execute.php\n\nSite::$onBeforeScriptExecute = function() {\n    sleep(5);\n};\n</code></pre> <p>With this change loaded up into your local studio, every request that isn\u2019t a static asset request will gain a 5 second delay upfront\u2014forcing every UI assertion in your test that accidentally depends on an XHR call finishing in less than 4 seconds to fail. Spend one round after you finish your new Cypress tests checking them with this activated, and you can gain a lot of confidence it will work as consistently in CI as it does locally. Just be sure to discard this change when you\u2019re done and avoid committing it with your work\u2014or add it to your local <code>.git/info/exclude</code> to keep it around entirely ignored by Git while you do your work.</p>"},{"location":"development/running-tests/fixtures/","title":"Adding fixture data","text":"<p>You can use this workflow for identifying and capturing fixture changes: - load existing fixture data by running <code>load-fixtures</code> at the studio command prompt - Take a complete snapshot of the database <code>dump-sql &gt; .scratch/snapshot.before-changes.sql</code> - Create new records using the UI - Take a complete snapshot <code>dump-sql &gt; .scratch/snapshot.after-changes.sql</code> - Open up the two .sql files in a visual diff viewer and manually transplant the added records over to their appropriate fixture files in the repo</p>"},{"location":"development/workspace-setup/content-editor/","title":"Content Editor webapp","text":""},{"location":"development/workspace-setup/content-editor/#code-layout","title":"Code layout","text":"<ul> <li><code>sencha-workspace/</code><ul> <li><code>packages/</code><ul> <li><code>emergence-cms/</code>: Primary location for content editor UI code</li> <li><code>emr-skeleton-theme/</code>: The Sencha theme used when rendering the content editor</li> </ul> </li> <li><code>EmergenceContentEditor/</code>: A thin Sencha application used to make development easier and to provide a build target for generating the theme</li> </ul> </li> <li><code>html-templates/</code><ul> <li><code>webapps/EmergenceContentEditor/sencha.tpl</code>: A template for rendering the content editor embedded in the site\u2019s design frame</li> <li><code>html-templates/blog/blogPostEdit.tpl</code>: A wrapper around <code>sencha.tpl</code> to provide the content editor UI on the blog post edit form</li> <li><code>html-templates/pages/pageEdit.tpl</code>: A wrapper around <code>sencha.tpl</code> to provide the content editor UI on the page edit form</li> </ul> </li> </ul>"},{"location":"development/workspace-setup/content-editor/#running-live-changes","title":"Running live changes","text":"<p>The frontend Sencha application needs to be built at least once with the Sencha CMD build tool to scaffold/update a set of loader files. After that, you can just edit files the working tree and reload the browser. The two exceptions where you need to build again are changing the list of packages or changing the list of override files.</p> <p>Before the frontend application can be built to run from live changes, you\u2019ll need to ensure all submodules are initialized:</p> <pre><code>git submodule update --init\n</code></pre> <p>Then, use the shortcut studio command for building the frontend application:</p> <pre><code>build-content-editor\n</code></pre> <p>Once built, the live-editable version of the app can be accessed via the static web server that the studio runs on port <code>{{ no such element: dict object['static_port'] }}</code>. The backend host must be provided to the apps via the <code>?apiHost</code> query parameter. Any remote backend with CORS enabled will work, or you can use the local backend:</p> <p><code>localhost:{{ no such element: dict object['static_port'] }}/EmergenceContentEditor/?apiHost=localhost:9070</code></p>"},{"location":"development/workspace-setup/content-editor/#working-with-breakpoints","title":"Working with breakpoints","text":"<p>By default, the Sencha framework will automatically append random cache buster values to every loaded <code>.js</code> source. This helps ensures that your latest code always runs, but will also prevent any breakpoints you set from persisting across reloads.</p> <p>With the Disable cache option of the network inspector activated, you can disable this built-in cache buster by appending <code>&amp;cache=1</code> to the current page\u2019s query string.</p>"},{"location":"development/workspace-setup/content-editor/#connecting-to-remote-server","title":"Connecting to remote server","text":"<p>You can connect to any remote instance that has CORS enabled by appending the query parameter <code>apiHost</code> when loading the page. If the remote instance requires HTTPS, append <code>apiSSL=1</code> as well.</p>"},{"location":"development/workspace-setup/local-studio/","title":"Local Studio Container","text":"<p>This guide will walk you through launching a Docker-container local development studio and using it to test changes made within a local Git repository.</p>"},{"location":"development/workspace-setup/local-studio/#launch-studio-container","title":"Launch studio container","text":"<ol> <li> <p>Install Chef Habitat:</p> <pre><code>curl -s https://raw.githubusercontent.com/habitat-sh/habitat/master/components/hab/install.sh | sudo bash\n</code></pre> </li> <li> <p>Set up Chef Habitat, accepting defaults for all prompts:</p> <pre><code>hab setup\n</code></pre> </li> <li> <p>Clone <code>laddr</code> repository and any submodules:</p> <pre><code>git clone --recursive git@github.com:CodeForPhilly/laddr.git\n</code></pre> </li> <li> <p>Change into cloned directory:</p> <pre><code>cd ./laddr\n</code></pre> </li> <li> <p>Launch studio:</p> <p>Use the included scripts-to-rules-them-all workflow script to configure and launch a studio session:</p> <pre><code>script/studio\n</code></pre> <p>Review the notes printed to your terminal at the end of the studio startup process for a list of all available studio commands.</p> </li> </ol>"},{"location":"development/workspace-setup/local-studio/#bootstrap-and-develop-backend","title":"Bootstrap and develop backend","text":"<ol> <li> <p>Start services:</p> <p>Use the studio command <code>start-all</code> to launch the http server (nginx), the application runtime (php-fpm), and a local mysql server:</p> <pre><code>start-all\n</code></pre> <p>At this point, you should be able to open localhost:9070 and see the error message <code>Page not found</code>.</p> </li> <li> <p>Build site:</p> <p>To build the entire site and load it, use the studio command <code>update-site</code>:</p> <pre><code>update-site\n</code></pre> <p>At this point, localhost:9070 should display the current build of the site</p> </li> <li> <p>Load fixture data into site database (optional):</p> <pre><code>load-fixtures\n</code></pre> <p>The standard fixture data includes the following users:</p> Username Password AccountLevel About <code>system</code> <code>system</code> <code>Developer</code> Full system access <code>admin</code> <code>admin</code> <code>Administrator</code> Manage site and staff <code>staff</code> <code>staff</code> <code>Staff</code> Staff access <code>user</code> <code>user</code> <code>User</code> Regular user </li> <li> <p>Make and apply changes:</p> <p>After editing code in the working tree, you must rebuild and update the site:</p> <pre><code>update-site\n</code></pre> <p>A command to automatically rebuild and update the site as changes are made to the working tree is also available, but currently not that efficient or reliable:</p> <pre><code>watch-site\n</code></pre> </li> </ol>"},{"location":"development/workspace-setup/local-studio/#enable-user-registration","title":"Enable user registration","text":"<p>To enable user registration on a site that comes with it disabled:</p> <pre><code># write class configuring enabling registration\nmkdir -p php-config/Emergence/People\necho '&lt;?php Emergence\\People\\RegistrationRequestHandler::$enableRegistration = true;' &gt; php-config/Emergence/People/RegistrationRequestHandler.config.php\n\n# rebuild environment\nupdate-site\n</code></pre> <p>After visiting <code>/register</code> and creating a new user account, you can use the studio command <code>promote-user</code> to upgrade the user account you just registered to the highest access level:</p> <pre><code>promote-user &lt;myuser&gt;\n</code></pre>"},{"location":"development/workspace-setup/local-studio/#connect-to-local-database","title":"Connect to local database","text":"<p>The studio container hosts a local MySQL instance that can be connected to at:</p> <ul> <li>Host: <code>localhost</code> (or LAN/WAN IP of machine hosting Docker engine)</li> <li>Port: <code>9076</code></li> <li>Username: <code>admin</code></li> <li>Password: <code>admin</code></li> </ul>"},{"location":"development/workspace-setup/virtual-multisite/","title":"Virtual Multi-site Container","text":""},{"location":"development/workspace-setup/virtual-multisite/#launch-virtual-multisite-container","title":"Launch virtual multisite container","text":"<pre><code>docker run \\\n--name emergence \\\n-v emergence:/emergence \\\n-p 80:80 \\\n-p 3306:3306 \\\n-p 9083:9083 \\\njarvus/emergence\n</code></pre>"},{"location":"getting-started/","title":"Getting Started","text":"<p>The Getting Started section provides content covering:</p> <ul> <li>Overviews of the system</li> <li>Obtaining a running instance of the system</li> <li>Configuring a new system for an organization</li> <li>Onboarding new users into the system</li> </ul>"},{"location":"operations/","title":"Operations","text":"<p>The Operations section provides content covering:</p> <ul> <li>Overviews of the physical infrastructure and service components</li> <li>Building hosted environments</li> <li>Maintaining hosted environments</li> <li>Backing up and restoring content</li> <li>Monitoring system health</li> </ul>"},{"location":"operations/update-saml2-certificate/","title":"Update SAML2 Certificate","text":"<p>The OpenSSL certificate used by Laddr\u2019s Single Sign-On (SSO) integration with Slack needs to be refreshed occasionally when it nears or passes its expiration date</p>"},{"location":"operations/update-saml2-certificate/#generate-a-new-certificate","title":"Generate a new certificate","text":"<p>On any computer with the <code>openssl</code> command installed (readily available on macOS and Linux), you can generate the new key+certificate pair before installing it to your Slack and Laddr instances:</p> <ol> <li> <p>Generate private key:</p> <pre><code>openssl genrsa \\\n-out ./laddr-slack-private-key.pem \\\n1024\n</code></pre> </li> <li> <p>Generate public certificate:</p> <pre><code>openssl req -new -x509 \\\n-days 1095 \\\n-key ./laddr-slack-private-key.pem \\\n-out ./laddr-slack-public-cert.pem\n</code></pre> <p>Fill out the prompts with appropriate information about your organization. These values don\u2019t really matter for anything</p> </li> <li> <p>If your Laddr instance is hosted on Kubernetes, encode the two generated files into a <code>Secret</code> manifest (you only need the <code>kubectl</code> command installed on your local system for this, it does not need to be connected to any cluster):</p> <pre><code>kubectl create secret generic saml2 \\\n--output=yaml \\\n--dry-run \\\n--from-file=SAML2_PRIVATE_KEY=./laddr-slack-private-key.pem \\\n--from-file=SAML2_CERTIFICATE=./laddr-slack-public-cert.pem \\\n&gt; ./saml2.secret.yaml\n</code></pre> </li> <li> <p>If your cluster uses sealed secrets, seal the newly-created secret:</p> <pre><code>export SEALED_SECRETS_CERT=https://sealed-secrets.live.k8s.phl.io/v1/cert.pem\nkubeseal \\\n--namespace \"my-project\" \\\n-f ./saml2.secret.yaml \\\n-w ./saml2.sealed-secret.yaml\n</code></pre> <p>Be sure to replace <code>my-project</code> with the namespace your instance is deployed within</p> </li> <li> <p>Deploy the sealed secret to your cluster</p> <p>In Code for Philly\u2019s case, that means updating <code>saml2.yaml</code> with the new content and then merging the generated deploy PR. After the deploy, you may need to delete the existing secret in order for the <code>sealed-secrets</code> operator to replace it with the updated secret</p> </li> <li> <p>Finally, visit https://my-org.slack.com/admin/auth/saml?sudo=1 and edit the Public Certificate, pasting the contents of <code>./laddr-slack-public-cert.pem</code>:</p> <pre><code>cat ./laddr-slack-public-cert.pem\n# paste output to Slack admin webpage\n</code></pre> <p>Slack will not let you save the new public certificate until it\u2019s been successfully applied to the host</p> </li> </ol>"},{"location":"usage/","title":"Usage","text":"<p>The Usage section provides content covering:</p> <ul> <li>Overviews of the system features</li> <li>Setting up and customizing a user account</li> <li>Using the system\u2019s features for day-to-day work</li> <li>Reporting and integration</li> </ul>"}]}